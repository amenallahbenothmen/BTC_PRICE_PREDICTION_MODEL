{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\amenm\\\\OneDrive\\\\Desktop\\\\Predecting_BTC_Price\\\\Bitcoin_predection_price'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir:Path\n",
    "    data_dir:Path\n",
    "    dataset_name:str\n",
    "    data_scaled_dir:Path\n",
    "    data_final_dir:Path\n",
    "    features:list[str]\n",
    "    look_back : int\n",
    "    forecast_horizon:int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LSTM_BTC_Prediction.constants  import *\n",
    "from src.LSTM_BTC_Prediction.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath=CONFIG_FILE_PATH,params_filepath=PARAMS_FILE_PATH):\n",
    "\n",
    "            self.config=read_yaml(config_filepath) \n",
    "            self.params=read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_spliting_config(self) -> DataPreprocessingConfig:\n",
    "        \n",
    "        config=self.config.data_preprocessing\n",
    "\n",
    "        create_directories([config.root_dir])  \n",
    "\n",
    "        data_preprocessing_config=DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            dataset_name= config.dataset_name,\n",
    "            features=config.features,\n",
    "            data_dir=config.data_dir,\n",
    "            data_scaled_dir=config.data_scaled_dir,\n",
    "            data_final_dir=config.data_final_dir,\n",
    "            look_back=self.params.LOOK_BACK,\n",
    "            forecast_horizon=self.params.FORECAST_HORIZON\n",
    "            )\n",
    "\n",
    "        return data_preprocessing_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from src.LSTM_BTC_Prediction import logger \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np \n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            file_path = os.path.join(self.config.root_dir, f\"{self.config.dataset_name}.csv\")\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            logger.info(f\"Data loaded successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during data loading for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def select_features(self):\n",
    "        try:\n",
    "            self.df = self.df[self.config.features]\n",
    "            logger.info(f\"Features selected successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during feature selection for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def split_dataset(self):\n",
    "        try:\n",
    "            self.df_train = self.df[:int(len(self.df) * 0.6)]\n",
    "            self.df_val = self.df[int(len(self.df) * 0.6):int(len(self.df) * 0.8)]\n",
    "            self.df_test = self.df[int(len(self.df) * 0.8):]\n",
    "            logger.info(f\"Dataset split successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during dataset splitting for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n",
    "    def transform_data(self):\n",
    "        try:\n",
    "            scaler_train = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler_val = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler_test = MinMaxScaler(feature_range=(0, 1))\n",
    "            self.df_train_scaled = scaler_train.fit_transform(self.df_train)\n",
    "            self.df_val_scaled = scaler_val.fit_transform(self.df_val)\n",
    "            self.df_test_scaled = scaler_test.fit_transform(self.df_test)\n",
    "            logger.info(f\"Data transformed successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during data transformation for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def dataset_generator_lstm(self, dataset: np.ndarray, look_back: int, forecast_horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset) - look_back - forecast_horizon + 1):\n",
    "            window_size_x = dataset[i:(i + look_back), :]\n",
    "            window_size_y = dataset[i + look_back:i + look_back + forecast_horizon, 0]\n",
    "            dataX.append(window_size_x)\n",
    "            dataY.append(window_size_y)\n",
    "        return np.array(dataX), np.array(dataY) \n",
    "\n",
    "    def transform_generator(self):\n",
    "        try:\n",
    "            self.trainX,self.trainY=self.dataset_generator_lstm(dataset=self.df_train_scaled, look_back=self.config.look_back, forecast_horizon=self.config.forecast_horizon)\n",
    "            self.valX,self.valY=self.dataset_generator_lstm(dataset=self.df_val_scaled, look_back=self.config.look_back, forecast_horizon=self.config.forecast_horizon)\n",
    "            self.testX,self.testY=self.dataset_generator_lstm(dataset=self.df_test_scaled, look_back=self.config.look_back, forecast_horizon=self.config.forecast_horizon)\n",
    "            logger.info(f\"Data transformation and generation completed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during data transformation and generation: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_final_dataset(self): \n",
    "        try:\n",
    "            os.makedirs(\"artifacts/data_final\", exist_ok=True) \n",
    "            np.save(os.path.join(self.config.data_final_dir, \"trainX.npy\"), self.trainX)\n",
    "            np.save(os.path.join(self.config.data_final_dir, \"trainY.npy\"), self.trainY)\n",
    "            np.save(os.path.join(self.config.data_final_dir, \"valX.npy\"), self.valX)\n",
    "            np.save(os.path.join(self.config.data_final_dir, \"valY.npy\"), self.valY)\n",
    "            np.save(os.path.join(self.config.data_final_dir, \"testX.npy\"), self.testX)\n",
    "            np.save(os.path.join(self.config.data_final_dir, \"testY.npy\"), self.testY)\n",
    "            logger.info(f\"Final datasets saved successfully at: {self.config.data_final_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during saving final datasets: {e}\")\n",
    "            raise e  \n",
    "\n",
    "    def save_dataset_splited(self):\n",
    "        try:\n",
    "            os.makedirs(\"artifacts/data_split\", exist_ok=True)\n",
    "            file_path_train = os.path.join(self.config.data_dir, f\"train.csv\")\n",
    "            self.df_train.to_csv(file_path_train)\n",
    "            file_path_val = os.path.join(self.config.data_dir, f\"val.csv\")\n",
    "            self.df_val.to_csv(file_path_val)\n",
    "            file_path_test = os.path.join(self.config.data_dir, f\"test.csv\")\n",
    "            self.df_test.to_csv(file_path_test)\n",
    "            logger.info(f\"Splited dataset saved successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during saving splited dataset for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_dataset_splited_transformed(self):\n",
    "        try:\n",
    "            os.makedirs(\"artifacts/data_scaled\", exist_ok=True)\n",
    "            file_path_train = os.path.join(self.config.data_scaled_dir, f\"train_scaled.csv\")\n",
    "            pd.DataFrame(self.df_train_scaled).to_csv(file_path_train)\n",
    "            file_path_val = os.path.join(self.config.data_scaled_dir, f\"val_scaled.csv\")\n",
    "            pd.DataFrame(self.df_val_scaled).to_csv(file_path_val)\n",
    "            file_path_test = os.path.join(self.config.data_scaled_dir, f\"test_scaled.csv\")\n",
    "            pd.DataFrame(self.df_test_scaled).to_csv(file_path_test)\n",
    "            logger.info(f\"Splited and transformed dataset saved successfully for {self.config.dataset_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred during saving splited and transformed dataset for {self.config.dataset_name}: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-06 15:19:03,798: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-06 15:19:03,800: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-06 15:19:03,801: INFO: common: created directory at: artifacts]\n",
      "[2024-05-06 15:19:03,801: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2024-05-06 15:19:03,825: INFO: 2674098266: Data loaded successfully for BTC]\n",
      "[2024-05-06 15:19:03,827: INFO: 2674098266: Features selected successfully for BTC]\n",
      "[2024-05-06 15:19:03,827: INFO: 2674098266: Dataset split successfully for BTC]\n",
      "[2024-05-06 15:19:03,834: INFO: 2674098266: Data transformed successfully for BTC]\n",
      "[2024-05-06 15:19:03,846: INFO: 2674098266: Data transformation and generation completed successfully]\n",
      "[2024-05-06 15:19:03,878: INFO: 2674098266: Splited dataset saved successfully for BTC]\n",
      "[2024-05-06 15:19:03,916: INFO: 2674098266: Splited and transformed dataset saved successfully for BTC]\n",
      "[2024-05-06 15:19:03,936: INFO: 2674098266: Final datasets saved successfully at: artifacts/data_final]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config= ConfigurationManager()\n",
    "    config=config.get_data_spliting_config()\n",
    "    data_preprocessing = DataPreprocessing(config)\n",
    "    data_preprocessing.load_data()\n",
    "    data_preprocessing.select_features()\n",
    "    data_preprocessing.split_dataset()\n",
    "    data_preprocessing.transform_data()\n",
    "    data_preprocessing.transform_generator()\n",
    "    data_preprocessing.save_dataset_splited()\n",
    "    data_preprocessing.save_dataset_splited_transformed()\n",
    "    data_preprocessing.save_final_dataset()\n",
    "except Exception as e:\n",
    "    raise e        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1982, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
